\documentclass[12pt,a4j,twoside]{jarticle}

\usepackage{gradthesis}
\usepackage{amsmath}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amssymb}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\def\AgentSet{A}
\def\ChargeBase{B}
\def\Dreq{{D_{\it req}}}
\def\En{\mathcal{E}}
\def\SelfAss{{S_{\it ass}}}

\def\BatteryMax{{B_{\it max}}}
\def\BatteryLevel{b}

\def\HomingCheck{{T_{\it homing}}}
\def\HomingBattery{{k_{\it homing}}}
\def\PausingInt{{S_{\it pause}}}

\def\PauseTimeFactor{{\gamma_{\it p}}}

\def\DeactCheckStartTime{D_{\it st}}
\def\DeactCheckInterval{D_{\it int}}
\def\PauseCount{x_{\it pause}}
\def\DeactCount{N_{\it deact}}
\def\DeactThreshold{K_{\it deact}}



\def\green#1{\textcolor[rgb]{0,0.6,0}{#1}}
\def\dgreen#1{\textcolor[rgb]{0.02,0.50,0.05}{#1}}
\def\gray#1{\textcolor[rgb]{0.5,0.5,0.7}{#1}}
\def\red#1{\textcolor{red}{#1}}
\def\blue#1{\textcolor{blue}{#1}}
\def\Blue{\color[rgb]{0,0,1}}
\def\Red{\color[rgb]{1,0,0}}
\def\Dgreen{\color[rgb]{0.02,0.55,0.04}}
\def\Black{\color[rgb]{0,0,0}}
\def\ignore#1{}
\def\original#1{}

\title{マルチエージェント協調巡回問題におけるエネルギー消費抑制手法の提案}{5}
\etitle{hoge}
\author{松本~航平}
\studentid{1W193102}
\university{早稲田大学}
\faculty{基幹理工学部}
\department{情報理工学科}
\type{卒業論文}
\nendo{2022}
\hizuke{2023/}
\advisor{菅原~俊治}

\begin{document}
\maketitle

\begin{abstract}
  \red{後で追記する}
  \par
  
\end{abstract}

\vspace*{1cm}\par
\section{序論}
\label{sec:introduction}
\red{後で追記する}
\par

近年，ロボット技術が発達し，巡回パトロールや清掃などといった，人間が日常的に繰り返す作業や，
災害地や原子力発電所，宇宙などでの作業を複数の自律的ロボットで代替する動きが加速している．
このような複数のロボットが協調して共通の作業を行う問題は，ロボットを自律的に動作するエージェントとしてモデル化し，
マルチエージェント協調巡回問題({\em multi-agent cooperative patrolling problem}, MACPP)と呼ばれる．
MACPPの研究では，複数のエージェントが協力・協調することで，
与えられた環境において効率的かつ効果的に巡回を行うための方法・アルゴリズムを見出すことを目的としている．
\par

巡回効率だけを追求した高度な行動や学習は，確かに巡回の効率は向上するものの，
必要以上にエネルギーを消費する可能性があり，これはMACPPの極めて大きな課題となっている．
特に，本研究で想定する自律型エージェントは独自のバッテリを持ち，頻繁な充電を強いられることになる．
一方，アプリケーションによっては，巡回作業に対する品質要求があり，
それを超えることは必ずしも期待されているわけではない．
例えば，清掃問題では，ある程度環境がきれいになっていれば十分であり，
過度の巡回作業はかえって単位エネルギー当たりの作業効率を低下させる．
また，環境が複雑で大規模な場合，どれだけのエージェントが必要かを事前に判断することができないこともあり，
エージェントが少ないと品質要求を満たせず，多すぎるとエネルギーの浪費に繋がる．
\par

マルチエージェントシステムにおける協調の観点からエネルギー効率に注目した研究\cite{Kim2016,Benkrid2019,Notomista2022}がある．
例えば，\cite{Benkrid2019}では，マルチロボットにおける探索問題において，
移動ロボットの総運動エネルギーを節約するために，運動時間を短縮する分散型協調手法を提案している．
しかし，これらの研究は，タスクのために効率的に移動・作業し，
結果として総消費エネルギーを削減する方法を目指したものである．
一方，本手法では，他のエージェントが要求されるタスクの質を満たすことができれば，
あるエージェントが自らの判断でしばらく停止したり，システムから退出したりすることを実現しようとするものである．
また，\cite{WuPrima2019, Wu2019}では，エージェントが要求を満たせば自律的に一時停止してエネルギーを節約し，
再び充電して探索に戻るという手法を提案している，
しかし，その方法は不十分であり，エージェントは依然として不必要に環境内を巡回していることが分かった．
\par

そこで本研究では，\cite{Wu2019}の手法を拡張し，その後の行動によって起こりうる貢献を自律的に予測し，
環境の現状を推定することで要求品質の全体的な達成度を把握しながら，
より効率的に要求品質の充足とエネルギー消費の削減を両立する手法を提案する．
主な違いは，エージェントが休止・充電している間の巡回タスクの進捗は，
他のエージェントの行動に依存し，エージェントごとに異なるため，
個々のエージェントから見た省エネルギー行動の自律学習を導入しているという点である．
\par

さらに，この学習が進むとエージェントは，短い休止時間で巡回するBusyグループと，
比較的長い時間休止してエネルギーを消費しないEnergy saveグループに分かれることを発見した．
そのため，後者のグループのエージェントは，品質要求を満たしたまま，順次，活動を停止することができる，
実験の結果，提案手法により，従来手法と比べて大幅にエネルギー消費量を削減することができることが分かった．
また，Busyグループに属するエージェントの数は，環境条件によって変化することが分かった．
その後，品質要求を満たしつつ，Energy saveグループに属するエージェントを順次停止することで，
活動しているエージェント数を削減することができた．
\par

\section{関連研究}
\red{後で，杉山さんの研究も追加する}

MACPPとその応用に関する研究は数多く行われている\cite{Hattori2021,Tevyashov2022,Wiandt2018,Othmani2017,Zhou2019}．
例えば， \cite{Othmani2017}では，{\em edge Markov evolution graphs}を用いた動的環境変化のモデルを提案した．
また，\cite{Zhou2019}では，巡回問題をベイズ適応型遷移分離部分観測可能マルコフ決定過程として定式化し，
モンテカルロ木探索法を拡張した分散型オンライン学習・計画アルゴリズムを提案している．
さらに，\cite{Othmani2018}では，人口ニューラルネットワークを用いて，
個人のidlnessから共用のidlnessを予測する方法を提案している．
しかし，これらの研究では，バッテリの放電やエネルギー消費による定期的な充電を無視し，
効率性のみを考慮したものである．
\cite{Yoneda2013}では，Q学習によって決定される計画戦略のもと，
複数のエージェントが定期的に充電しながら協調して巡回するAMTDS({\em adaptive meta-target decision strategy})という手法を提案している．
さらに，\red{ここから杉山さんの研究}．
しかし，この研究もエネルギー消費量の削減を考慮せず，巡回の効率化を目的とした学習のみを行うものである．
\par

一方，\cite{Kim2016,Benkrid2019,Notomista2022,Kim2016,Wu2019,Latif2021}では一部，省エネルギーに着目している．
\cite{Kim2016}では，消防ロボット以外にも複数のサブロボットを導入し，消火作業における総作業時間の延長を図った．
また，\cite{Latif2021}では，協調型群ロボットに対して，
採餌などの連続タスクを解決するためのエネルギー配慮型分散タスク配分アルゴリズムを提案し，
高効率なミッションの実現を実現した．
しかし，これらの研究では，本研究とは異なり，動作時間の延長も考慮されている．
これに対して，\cite{Wu2019}では，AMTDSを巡回の品質要求に合わせて拡張し，省エネルギー化を図った．
しかし，この手法によるエネルギー削減は不十分であり，エージェントの活動にはまだ不要な行動が含まれている．
そこで，本研究では，個々の視点からの学習を導入することで，品質要求を満たしながら，さらにエネルギー消費を削減した．
さらに，複数のエージェントを停止する手法を提案し，消費エネルギーの削減を図った．

\section{モデルの定義}

\subsection{環境}

\subsection{エージェント}

\subsection{評価指標}

\section{準備}

\subsection{Adaptive meta target decision strategy (AMTDS)}
\label{subsec:AMTDS}

\subsubsection{目標決定戦略}
\label{target_strategy}

\begin{description}
  \item[Random selection (R)]\mbox{}\\
  環境全体のノード集合$V$からランダムに$v^i_{tar}$を選ぶ.

  \item[Probabilistic greedy selection (PGS)]\mbox{} \\
  環境全体のノード集合$V$内のノード$v$におけるイベント発生量の推定値$EL^i_t(v)$の上位$N_g$個のノードから,ランダムに1つ$v^i_{tar}$を選ぶ.
  この際に,学習や訪問をする$v^i_{tar}$の偏りを防ぐため,$N_g$番目のノードと$EL^i_t(v)$の値が同じノードが存在する場合,
  そのノードをすべて含めた後,その中から$v^i_{tar}$をランダムに選んでいる.

  \item[Prioritizing unvisited interval (PI)]\mbox{} \\
  環境全体のノード集合$V$内のノード$v$における訪問間隔$I^i_t(v)$の上位$N_i$個のノードから,ランダムに1つ$v^i_{tar}$を選ぶ.
  この際に,学習や訪問をする$v^i_{tar}$の偏りを防ぐため,$N_i$番目のノードと$I^i_t(v)$の値が同じノードが存在する場合,
  そのノードをすべて含めた後,その中から$v^i_{tar}$をランダムに選んでいる.
  
  \item[Balanced neighbor-preferential selection (BNPS)]\mbox{} \\
  近隣のノードにイベント発生量が多いとエージェントが判断したとき,近隣を優先的に巡回する.
  $v^i_{tar}$の決定時にエージェントの現在地$v^i_t$との距離が$d_{rad}$以下のノード集合を近領域$V^i_{area}$とする.
  ここで,$V^i_{area}$における1ステップあたりのイベント処理量の期待値$EV^i_t$は以下の式で求められる.
  \begin{equation}
    EV^i_t = \frac{\displaystyle \sum_{v \in V^i_{area}}EL^i_t(v)}{|V^i_{area}|}  
  \end{equation}
  エージェント$i$は近領域内のイベントを処理するか判断するための閾値$EV_{threshold}$と$EV^i_t$の値を比較し,
  $EV^i_t > EV_{threshold}$の間はPGSによって近領域内から$v^i_{tar}$を選ぶ.
  その後,$EV^i_t \le EV_{threshold}$となった場合,環境全体を対象とし,PGSで$v^i_{tar}$を選ぶ.
  環境全体から$v^i_{tar}$を選択した後,$V^i_{area}$を更新する.
  更新後の$V^i_{area}$の1ステップあたりのイベント処理量の期待値を$EV^i_{t+1}$とし,$EV_{threshold}$の値を以下の式に従って更新する.
  \begin{equation}
    EV_{threshold} \gets EV_{threshold} + \alpha(EV^i_{t+1} - EV_{threshold})
  \end{equation}
  ここで,$\alpha(0 < \alpha < 1)$は学習率である.また,$EV_{threshold}$の初期値は初めに$V^i_{area}$を設定した際の$EV^i_t$の値である.
\end{description}

\subsubsection{経路生成戦略}
  \label{route_strategy}
  
  \subsection{AMTDS with learning of dirt accumulation (AMTDS/LD)}
  
  \subsection{AMTDS for energy saving and cleanliness (AMTDS/ESC)}
  
  \subsubsection{要求充足の判断}
 
  \subsubsection{自己重要度評価}
  
  \subsubsection{帰還動作 (Homing)}
  
  \subsubsection{待機動作 (Pausing)}
  
  \section{提案手法}
  
  \subsection{AMTDS for energy saving under the requirement (AMTDS/ER)}
  
  \subsubsection{HomingとPausingの組み合わせ}
  
  \subsubsection{補正係数$K$の導入}
  
  \subsubsection{未来のイベント発生量の予測}
  
  \subsection{AMTDS for energy saving under the requirement with learning of  event probabilities (AMTDS/ERL)}
  
  \subsubsection{イベント発生量の予測に使用するノードの範囲の変更}
  
  \subsubsection{補正係数$K^i$の更新方法の変更}
  
  \section{評価実験}

  \subsection{実験環境}
  
  \begin{table}
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{エージェントに関するパラメータ}
      \begin{tabular}{lcr} \\ \hline
        種類 & パラメーター & 値 \\ \hline
        エージェント数 &  |A| & 20 \\ \hline
        バッテリ & $B^i_{max}$ & 900 \\
                   & $B^i_{drain}$ & 1 \\
                   & $k^i_{charge}$ & 3 \\ \hline
        経路生成戦略 & $d_{myopia}$ & 10 \\
                     & $k_{att}$ & 1.0 \\
                     & $k_{rover}$ & 1.2 \\ \hline
      \end{tabular}
      \label{tb:1}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{目標決定戦略のパラメーター}
      \begin{tabular}{lcrr} \\ \hline
        目標決定戦略 & パラメーター & 値 \\ \hline
        PGS & $N_g$ & 5 \\ \hline
        PI & $N_i$ & 5 \\ \hline
        BNPS & $\alpha$ & 0.1 \\
             & $d_{rad}$ & 15 \\ \hline
        AMTDS & $\alpha$ & 0.1 \\
              & $\varepsilon$ & 0.05 \\ \hline
        AMTDS/LD & $\beta$ & 0.1 \\ \hline 
      \end{tabular}
      \label{tb:2}
    \end{minipage}
    %
    \vskip\baselineskip
    %
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{エネルギー節約行動に関するパラメーター}
      \begin{tabular}{lcrr} \\ \hline
        種類 & パラメーター & 値 \\ \hline
        自己重要度評価 & $T_s$ & 20 \\
                       & $T_l$ & 50 \\
                       & $T_f$ & 10 \\ \hline
        Homing & $T_{check}$ & 100 \\
               & $k_{homing}$ & $\dfrac{1}{3}$ \\ \hline
        Pausing & $T_{basic}$ & 100 \\ \hline  
        AMTDS/ERL & $T_{hp}$ & 500,000 \\ \hline       
      \end{tabular}
      \label{tb:3}
    \end{minipage}
  \end{table}

  \subsection{AMTDS/ERについての実験結果}
  \label{result_ER}
  
  \subsubsection{実験1: 性能評価}
  \label{ex:ER1}
  
  \subsubsection{実験2: 環境の変化による性能の違い}
  \label{ex:ER2}

  \subsubsection{実験3: エージェント数減少による性能の変化}

  \subsubsection{実験4: $K^i$の降順にエージェント数を減少させたときの性能の変化}

  \subsection{AMTDS/ERCについての実験結果}
  \label{result_ERC}
  
  \subsubsection{実験5: 性能評価}
  \label{ex:ERC1}
  
  \subsubsection{実験6: 環境の変化による性能の違い}
  \label{ex:ERC2}  
  
  \subsubsection{実験7: エージェント数減少による性能の変化}

  \subsubsection{実験8: $K^i$の降順にエージェント数を減少させたときの性能の変化}

  \section{結論}

  \clearpage
  \bibliographystyle{junsrt}
  \bibliography{ref}

\end{document}