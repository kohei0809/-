\documentclass[12pt,a4j,twoside]{jarticle}

\usepackage{gradthesis}
\usepackage{amsmath}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amssymb}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\def\AgentSet{A}
\def\Dreq{{D_{\it req}}}
\def\En{\mathcal{E}}
\def\SelfAss{{S_{\it ass}}}

\def\BatteryMax{{B_{\it max}}}
\def\BatteryLevel{b}

\def\HomingCheck{{T_{\it homing}}}
\def\HomingBattery{{k_{\it homing}}}
\def\PausingInt{{S_{\it pause}}}

\def\PauseTimeFactor{{\gamma_{\it p}}}

\def\DeactCheckStartTime{D_{\it st}}
\def\DeactCheckInterval{D_{\it int}}
\def\PauseCount{x_{\it pause}}
\def\DeactCount{N_{\it deact}}
\def\DeactThreshold{K_{\it deact}}



\def\green#1{\textcolor[rgb]{0,0.6,0}{#1}}
\def\dgreen#1{\textcolor[rgb]{0.02,0.50,0.05}{#1}}
\def\gray#1{\textcolor[rgb]{0.5,0.5,0.7}{#1}}
\def\red#1{\textcolor{red}{#1}}
\def\blue#1{\textcolor{blue}{#1}}
\def\Blue{\color[rgb]{0,0,1}}
\def\Red{\color[rgb]{1,0,0}}
\def\Dgreen{\color[rgb]{0.02,0.55,0.04}}
\def\Black{\color[rgb]{0,0,0}}
\def\ignore#1{}
\def\original#1{}

\title{マルチエージェント協調巡回問題におけるエネルギー消費抑制手法の提案}{5}
\etitle{hoge}
\author{松本~航平}
\studentid{1W193102}
\university{早稲田大学}
\faculty{基幹理工学部}
\department{情報理工学科}
\type{卒業論文}
\nendo{2022}
\hizuke{2023/}
\advisor{菅原~俊治}

\begin{document}
\maketitle

\begin{abstract}
  \red{後で追記する}
  \par
  
\end{abstract}

\vspace*{1cm}\par
\section{序論}
\label{sec:introduction}
\red{後で追記する}
\par

近年，ロボット技術が発達し，巡回パトロールや清掃などといった，人間が日常的に繰り返す作業や，
災害地や原子力発電所，宇宙などでの作業を複数の自律的ロボットで代替する動きが加速している．
このような複数のロボットが協調して共通の作業を行う問題は，ロボットを自律的に動作するエージェントとしてモデル化し，
マルチエージェント協調巡回問題({\em multi-agent cooperative patrolling problem}, MACPP)と呼ばれる．
MACPPの研究では，複数のエージェントが協力・協調することで，
与えられた環境において効率的かつ効果的に巡回を行うための方法・アルゴリズムを見出すことを目的としている．
\par

巡回効率だけを追求した高度な行動や学習は，確かに巡回の効率は向上するものの，
必要以上にエネルギーを消費する可能性があり，これはMACPPの極めて大きな課題となっている．
特に，本研究で想定する自律型エージェントは独自のバッテリを持ち，頻繁な充電を強いられることになる．
一方，アプリケーションによっては，巡回作業に対する品質要求があり，
それを超えることは必ずしも期待されているわけではない．
例えば，清掃問題では，ある程度環境がきれいになっていれば十分であり，
過度の巡回作業はかえって単位エネルギー当たりの作業効率を低下させる．
また，環境が複雑で大規模な場合，どれだけのエージェントが必要かを事前に判断することができないこともあり，
エージェントが少ないと品質要求を満たせず，多すぎるとエネルギーの浪費に繋がる．
\par

マルチエージェントシステムにおける協調の観点からエネルギー効率に注目した研究\cite{Kim2016,Benkrid2019,Notomista2022}がある．
例えば，\cite{Benkrid2019}では，マルチロボットにおける探索問題において，
移動ロボットの総運動エネルギーを節約するために，運動時間を短縮する分散型協調手法を提案している．
しかし，これらの研究は，タスクのために効率的に移動・作業し，
結果として総消費エネルギーを削減する方法を目指したものである．
一方，本手法では，他のエージェントが要求されるタスクの質を満たすことができれば，
あるエージェントが自らの判断でしばらく停止したり，システムから退出したりすることを実現しようとするものである．
また，\cite{WuPrima2019, Wu2019}では，エージェントが要求を満たせば自律的に一時停止してエネルギーを節約し，
再び充電して探索に戻るという手法を提案している，
しかし，その方法は不十分であり，エージェントは依然として不必要に環境内を巡回していることが分かった．
\par

そこで本研究では，\cite{Wu2019}の手法を拡張し，その後の行動によって起こりうる貢献を自律的に予測し，
環境の現状を推定することで要求品質の全体的な達成度を把握しながら，
より効率的に要求品質の充足とエネルギー消費の削減を両立する手法を提案する．
主な違いは，エージェントが休止・充電している間の巡回タスクの進捗は，
他のエージェントの行動に依存し，エージェントごとに異なるため，
個々のエージェントから見たエネルギー節約行動の自律学習を導入しているという点である．
\par

さらに，この学習が進むとエージェントは，短い休止時間で巡回するBusyグループと，
比較的長い時間休止してエネルギーを消費しないEnergy saveグループに分かれることを発見した．
そのため，後者のグループのエージェントは，品質要求を満たしたまま，順次，活動を停止することができる，
実験の結果，提案手法により，従来手法と比べて大幅にエネルギー消費量を削減することができることが分かった．
また，Busyグループに属するエージェントの数は，環境条件によって変化することが分かった．
その後，品質要求を満たしつつ，Energy saveグループに属するエージェントを順次停止することで，
活動しているエージェント数を削減することができた．
\par

\section{関連研究}
\red{後で，杉山さんの研究も追加する、領域分解についても追加(SMASH参考)}

MACPPとその応用に関する研究は数多く行われている\cite{Hattori2021,Tevyashov2022,Wiandt2018,Othmani2017,Zhou2019}．
例えば， \cite{Othmani2017}では，{\em edge Markov evolution graphs}を用いた動的環境変化のモデルを提案した．
また，\cite{Zhou2019}では，巡回問題をベイズ適応型遷移分離部分観測可能マルコフ決定過程として定式化し，
モンテカルロ木探索法を拡張した分散型オンライン学習・計画アルゴリズムを提案している．
さらに，\cite{Othmani2018}では，人口ニューラルネットワークを用いて，
個人のidlnessから共用のidlnessを予測する方法を提案している．
しかし，これらの研究では，バッテリの放電やエネルギー消費による定期的な充電を無視し，
効率性のみを考慮したものである．
\cite{Yoneda2013}では，Q学習によって決定される計画戦略のもと，
複数のエージェントが定期的に充電しながら協調して巡回するAMTDS({\em adaptive meta-target decision strategy})という手法を提案している．
さらに，\red{ここから杉山さんの研究}．
しかし，この研究もエネルギー消費量の削減を考慮せず，巡回の効率化を目的とした学習のみを行うものである．
\par

一方，\cite{Kim2016,Benkrid2019,Notomista2022,Kim2016,Wu2019,Latif2021}では一部，省エネルギーに着目している．
\cite{Kim2016}では，消防ロボット以外にも複数のサブロボットを導入し，消火作業における総作業時間の延長を図った．
また，\cite{Latif2021}では，協調型群ロボットに対して，
採餌などの連続タスクを解決するためのエネルギー配慮型分散タスク配分アルゴリズムを提案し，
高効率なミッションの実現を実現した．
しかし，これらの研究では，本研究とは異なり，動作時間の延長も考慮されている．
これに対して，\cite{Wu2019}では，AMTDSを巡回の品質要求に合わせて拡張し，省エネルギー化を図った．
しかし，この手法によるエネルギー削減は不十分であり，エージェントの活動にはまだ不要な行動が含まれている．
そこで，本研究では，個々の視点からの学習を導入することで，品質要求を満たしながら，さらにエネルギー消費を削減した．
さらに，複数のエージェントを停止する手法を提案し，消費エネルギーの削減を図った．

\section{モデルの定義}
\label{sec:MACPP_model}
本研究は，\cite{Wu2019}で提案された清掃問題におけるエネルギー節約手法である，
{\em adaptive meta-target decision strategy for energy saving and cleanliness} (AMTDS/ESC)
の拡張である．
また，従来手法との比較を行うため，本研究で用いる問題の定式化と環境，エージェントの活動モデルは
\cite{Wu2019}で用いられているものを踏襲する．
\par

\subsection{環境}
エージェントの巡回環境を，2次元ユークリッド空間に埋め込み可能なグラフ$G = (V,E)$で表す．
ここで，$V = \{v_1, \dots, v_n \}$はノード集合を表し，
各ノード乗にエージェントやイベント，障害物が存在する．
また，$E$はエージェントが移動する経路に対応するノード$v_i$と$v_j$間のエッジ$e_{i,j}$である．
\par

さらに，ステップを単位とする離散時間を導入する．
簡単のため，必要に応じてダミーノードを追加することで，全てのエッジの長さは1に保たれる．
したがって，エージェントは1ステップで障害物のない隣接ノードに移動することができる．
ここで，$v_i$と$v_j$の最短距離(エッジの数)を$d(v_i,v_j)$とする．
\par

全てのノード$v\in V$上でイベントが発生し，そのイベント発生確率を$p(v)~(0\leq p(v)\leq 1)$とする．
毎時刻$t$において，ノード$v$に蓄積されたイベント数$L_t(v)$は以下の式で更新される．
%
\[
L_t(v) = \left\{
\begin{array}{ll}
  L_{t-1}(v) + 1 & \textrm{(確率$p(v)$のイベント発生時)} \\
  L_{t-1}(v) & \textrm{(その他)}
\end{array}
\right.
\]
%
時刻$t$にエージェントがノード$v$を訪れた時に$v$上のイベントは処理され，$L_t(v) = 0$となる．
イベントの解釈は，アプリケーションによって異なり，例えば，掃除のアプリケーションでは，
$p(v)$は場所$v$の汚れやすさを，$L_t(v)$は汚れの蓄積度合いを表す．
また，防犯監視パトロールのアプリケーションでは，$p(v)$はアプリケーションの所有者が指定した，
重要な場所に対する必要な防犯度合いを示し，$L_t(v)$は警戒レベルと解釈することができる．
本研究では，全ノードの$p(v)$はあらかじめ指定されていると仮定する．


\subsection{エージェント}
$n$個のエージェントの集合を$\AgentSet=\{1,\dots ,n\}$と表す．
エージェント$i\in\AgentSet$はバッテリを持ち，充電基地$v^i_{base}$で充電を繰り返すことで連続動作が可能である．
つまり，バッテリが満タンの状態で$v^i_{base}i$を出発した$i$は，環境を巡回し，
再び$v^i_{base}$に戻ってくるという動作をする．
エージェント$i$のバッテリ性能を$(B^i_{max}, B^i_{drain}, k^i_{charge})$で表す．
ここで，$B^i_{max}$はエージェントのバッテリ容量，$B^i_{drain}$は1ステップで消費するバッテリ消費量，
$k^i_{charge}$はバッテリ残量を1増加させるために必要なステップ数である．
時刻$t$におけるエージェント$i$のバッテリ残量を$\BatteryLevel_i (0 \leq \BatteryLevel_i \leq B_{max})$とすると，
$i$が1ステップで隣接するノードに移動したとき，$\BatteryLevel_i(t)$は以下の式に従って更新される．
%
\begin{equation}
  \BatteryLevel_i(t+1) \gets \BatteryLevel_i(t) - B^i_{drain}
\end{equation}
%
\par

$b^i(t)$が0になるとそのエージェントは移動できなくなってしまうので，自身のバッテリ残量が0になる前に戻らなければならない．
そこで，以下の式に示すように，エージェント$i$はノード$v$から充電基地$v^i_{base}$までの移動に必要な最小バッテリ量であるポテンシャル$\mathcal{P}^i(v)$を計算する．
%
\begin{equation}
  \mathcal{P}^i(v) = d(v, v^i_{base}) \times B^i_{drain}
\end{equation}
%
エージェント$i$は目標ノード$v^i_{tar}$を後の章で説明する目標決定戦略によって決定した際，実際に移動する前に，
現在のバッテリ残量で$v^i_{tar}$に到達し，その後充電基地に戻ることができるかを，以下の式を用いて判定する．
%
\begin{equation}
  b^i(t) \leq \mathcal{P}^i(v) + d(v^i_t, v^i_{tar}) \times B^i_{drain}
\end{equation}
%
この条件を満たさない場合，以下のように目標ノード$v^i_{tar}$を更新し，充電基地に戻る．
%
\begin{equation}
  v^i_{tar} \gets v^i_{base}
\end{equation}
%
エージェントは$v^i_{base}$に到着後，バッテリ残量が最大になるまで充電し，充電完了後は再び環境を巡回する．
ここで，満充電(つまり，$\BatteryLevel_i=\BatteryMax$)になるまでに，
$(\BatteryMax-\BatteryLevel_i)\times k_{charge}$ステップかかる．
\par

エージェント$i$は，すべてのノード$v$に対し，$v$のイベント発生確率の予測値を表す重要度$p^i(v) (0 \leq p^i(v) \leq 1)$を持つ．
$p^i(v)$は各エージェントが独立して保持しており，その値はエージェントごとに異なる．
$i$が$v$上にいない場合，現在の蓄積イベント数$L_t(v)$は知ることができない．
そこで，エージェントは時刻$t$の$p(v)$から期待値$E^i(L_t(v))$を以下の式に従って計算する．
%
\begin{equation}
  EL^i_t(v) = p^i(v) \times (t - t^v_{vis})
\end{equation}
%
この計算のために，エージェントは自分と他のエージェントの位置を知ることができると仮定する．
これは，現在の技術で容易に実現可能であるためである．
例えば，赤外線やGPSなどのセンサーを用いたり，エージェント間で直接通信したり，
クラウドロボティクス，すなわち，クラウドを介して情報を共有したりすることで実現できる．
しかし，エージェントは目的地を設定するための戦略や，目的地までの計画経路など，
エージェント内部の情報や判断を共有・推論することはできない．
\par

また，エージェント$i$と$j$はエージェント間の情報交換や交渉を用いる際に，互いに通信可能である．
しかし，エージェントの過度の通信によるコスト増加や干渉を防ぐために，通信可能範囲$d_{co}(>0)$が存在する．
$i$と$j$が以下の式を満たすとき，互いに通信可能である．
%
\begin{equation}
  m(v^i, v^j) < d_{co}
\end{equation}
%
また，同様の目的から，時間面の制約である最低通信間隔$B(>0)$が存在する．
$i$は$j$と最後に通信した時刻$T^{i, j}_{lst}$を保持しており，$T^{i, j}_{lst} + B$まで通信を行うことはできない．
\par

さらに，本研究では，要求品質を満たしつつエネルギー節約行動を学習することに重点を置いており，
実験で用いたグリッド状の環境では，衝突を回避する迂回経路の生成が容易であると考えられるため，
エージェント間の衝突は考慮しないこととする．
\cite{Yamauchi2022,Satish2017}のように，衝突が発生しない経路を生成するアルゴリズムはいくつか提案されており，
これらのアルゴリズムの1つを衝突回避に用いることができる．
\par

\subsection{評価指標}
評価指標は，扱うMACPPの種類によって異なるが，本研究では評価指標として，
以下の式で定められるイベント残存時間の総和$D_{t_s,t_e}$と，エージェントの総エネルギー消費量$C_{t_s,t_e}$を用いる．
%
\begin{align}
  D_{t_s,t_e} &= \sum_{v \in V} \sum^{t_e}_{t=t_s+1} L_t(v)\\
  C_{t_s,t_e} &= \sum_{i \in \AgentSet} \sum^{t_e}_{t=t_s+1} \En_t(i),
\end{align}
%
ここで，$[t_s,t_e]~(t_s < t_e)$ は時間間隔を表し，
$\En_t(i)$は$t$におけるエージェント$i$の消費エネルギーを表す．
したがって，$i$が隣接ノードに移動したとき$\En_t(i)=1$，
それ以外は$\En_t(i)=0$となる．
例えば，$D_{t_s,t_e}$は清掃問題において，掃除機をかけずに放置した埃の累積時間や，
セキュリティパトロールにおいて，チェックせずに放置したセキュリティ場所の累積時間や数を表し，
MACPPではこれを減らすことが目的となる．
\par
一般に，$D_{t_s,t_e}$と$C_{t_s,t_e}$の値はどちらも小さい方が良いとされるが，両者はトレードオフの関係である．
つまり，$D_{t_s,t_e}$と$C_{t_s,t_e}$の値をどちらも最小にすることは困難である．
したがって，\cite{Wu2019}と同様に1stepにおけるイベント量の要求値$\Dreq$を設定した．
エージェントは以下の式を満たせるように協調を行う．
%
\begin{equation}\label{eq:condition}
  D_{t_s,t_e}\leq \Dreq \times (t_e - t_s)
\end{equation}
%
本研究では，品質要求(式(\ref{eq:condition}))を満たしつつ，$C_{t_s, t_e}$をできるだけ小さくすることが目的である．
なお，本研究では単純化のため，これ以降，$D_{t_s,t_e}, C_{t_s,t_e}$を$D(s), C(s)$と表す．


\section{準備}
この章では，提案手法の基になったAMTDS，AMTDS/LD，AMTDS/EDC，AMTDS/ESCについて説明する．
これらの手法では，エージェントは目標ノード$v^i_{tar}$を決定する目標決定戦略と，それまでの経路を生成する経路生成戦略に従い，環境内を巡回する．
$v^i_{tar}$に到着した後，再び目標戦略に従って新しい目標ノードを決定するといったサイクルを各エージェントが繰り返し，継続的な環境巡回を行う．

\subsection{Adaptive meta target decision strategy (AMTDS)}
\label{subsec:AMTDS}
この節では，AMTDS/LD，AMTDS/ESCなどの手法のベースとなったAdaptive meta target decision strategy (AMTDS) \cite{Yoneda2013}について説明する．
AMTDSは単純な複数の目標決定戦略の中から，強化学習アルゴリズムであるQ学習によって，各エージェントが自身にとって最適な戦略を選択するメタ戦略学習である．
また，この手法では環境内のすべてのノード$v$におけるイベント発生確率$p(v)$は既知であるという仮定を導入しており，$p^i(v) = p(v)$とする．
\par

エージェント$i$はAMTDSによって目標決定戦略$s \in S$を選択し，$s$に従って目標ノード$v^i_{tar}$を決定する．
その後，経路生成戦略に従って$v^i_{tar}$に移動する．
ここで，$S$はエージェントが選択可能な目標決定戦略の集合である．
目標決定戦略については\ref{target_strategy}，経路生成戦略については\ref{route_strategy}で詳細を説明する．
$v^i_{tar}$に到着後，$v^i_{tar}$の決定時刻$t_0$から$d_{travel}$ステップ後に$v^i_{tar}$に到着するまでの1ステップあたりのイベント処理量を以下の式で計算する．
%
\begin{equation}
  \label{eq:reward_AMTDS}
  u_{t_0,t_0+d_{travel}} = \dfrac{\displaystyle\sum_{t_0+1 \leq t \leq t_0+d_{travel}} L_t(v^i_t)}{d_{travel}}
\end{equation}
%
さらに，これを報酬として，選択した戦略$s$のQ値$Q^i(s)$を以下の式に従って更新する．
%
\begin{equation}
  Q^i(s) \gets (1-\alpha)Q^i(s) + \alpha \times u_{t_0,t_0+d_{travel}}
\end{equation}
%
ここで，$\alpha (0 < \alpha \leq 1)$は学習率である．
$Q^i(s)$の更新後，$i$は次に選択する目標決定戦略$s_{next}$を$\varepsilon$-Greedy法によって決定する．
$\varepsilon$-Greedy法では，$s_{next}$を確率$\varepsilon$でランダムに選択し，確率$1 - \varepsilon$で以下の式に従って選択する．
%
\begin{equation}
  s_{next} \gets \argmax_{s} Q^i(s)
\end{equation}
%

\subsubsection{目標決定戦略}
\label{target_strategy}
\cite{Yoneda2013}では，各エージェントに以下の4つの基本的な目標決定戦略を$S$として与えている．
それぞれの戦略は単独でも使用可能な独立したものとなっている．
単独での使用を想定し，競合回避のためにランダム性を取り入れたものも存在する．
%
\begin{description}
  \item[Random selection (R)]\mbox{}\\
  環境全体のノード集合$V$からランダムに$v^i_{tar}$を選ぶ．

  \item[Probabilistic greedy selection (PGS)]\mbox{} \\
  環境全体のノード集合$V$内のノード$v$におけるイベント発生量の推定値$EL^i_t(v)$の上位$N_g$個のノードから，
  ランダムに1つ$v^i_{tar}$を選ぶ．
  この際に，学習や訪問をする$v^i_{tar}$の偏りを防ぐため，$N_g$番目のノードと$EL^i_t(v)$の値が同じノードが存在する場合，
  そのノードをすべて含めた後，その中から$v^i_{tar}$をランダムに選んでいる．

  \item[Prioritizing unvisited interval (PI)]\mbox{} \\
  環境全体のノード集合$V$内のノード$v$における訪問間隔$I^i_t(v)$の上位$N_i$個のノードから，ランダムに1つ$v^i_{tar}$を選ぶ．
  この際に，学習や訪問をする$v^i_{tar}$の偏りを防ぐため，$N_i$番目のノードと$I^i_t(v)$の値が同じノードが存在する場合，
  そのノードをすべて含めた後，その中から$v^i_{tar}$をランダムに選んでいる．
  
  \item[Balanced neighbor-preferential selection (BNPS)]\mbox{} \\
  近隣のノードにイベント発生量が多いとエージェントが判断したとき，近隣を優先的に巡回する．
  $v^i_{tar}$の決定時にエージェントの現在地$v^i_t$との距離が$d_{rad}$以下のノード集合を近領域$V^i_{area}$とする．
  ここで，$V^i_{area}$における1ステップあたりのイベント処理量の期待値$EV^i_t$は以下の式で求められる．
  \begin{equation}
    EV^i_t = \frac{\displaystyle \sum_{v \in V^i_{area}}EL^i_t(v)}{|V^i_{area}|}  
  \end{equation}
  エージェント$i$は近領域内のイベントを処理するか判断するための閾値$EV_{threshold}$と$EV^i_t$の値を比較し，
  $EV^i_t > EV_{threshold}$の間はPGSによって近領域内から$v^i_{tar}$を選ぶ．
  その後，$EV^i_t \le EV_{threshold}$となった場合，環境全体を対象とし，PGSで$v^i_{tar}$を選ぶ．
  環境全体から$v^i_{tar}$を選択した後，$V^i_{area}$を更新する．
  更新後の$V^i_{area}$の1ステップあたりのイベント処理量の期待値を$EV^i_{t+1}$とし，
  $EV_{threshold}$の値を以下の式に従って更新する．
  \begin{equation}
    EV_{threshold} \gets EV_{threshold} + \alpha(EV^i_{t+1} - EV_{threshold})
  \end{equation}
  ここで，$\alpha(0 < \alpha < 1)$は学習率である．
  また，$EV_{threshold}$の初期値は初めに$V^i_{area}$を設定した際の$EV^i_t$の値である．
\end{description}

\subsubsection{経路生成戦略}
  \label{route_strategy}
  経路生成戦略は{\em gradual path generation} (GPG)を用いる．
  GPGでは，まず$v^i_{tar}$までの最短経路をダイクストラ法を用いて生成し，
  その経路近辺でイベントが発生しやすいノードを経由するように経路を変更する．
  これにより，最短経路に従うよりも効率を高めることができる．
  しかし，経由するノードの増加によって$v^i_{tar}$への到着時間が遅れてしまい，逆に効率が下がってしまう．
  そのため，経由するノードに一定の制約をかけなければならない．
  そこで，GPGでは経由可能なノード$v$を以下の式を満たすものとする．
  %
  \begin{equation}
    \begin{cases}
      d(v^i_t, v) \leq d_{myopia} \\
      d(v, v^i_{tar}) < k_{att}(d(v^i_t, v^i_{tar})) \\
      d(v^i_t, v) + d(v, v^i_{tar}) \leq k_{rover}d(v^i_t, v^i_{tar}) \\
      \mathcal{P}^i(v^i_{tar}) + B^i_{drain} \times (d(v^i_t, v) + d(v, v^i_{tar})) \leq b^i(t)
    \end{cases}
  \end{equation}
  %
  ここで，$d_{myopia}$はエージェントが現在地とするノードから経由地点とできるノードまでの距離の閾値であり，
  $k_{att} (0 < k_{att} < 1)$は$v^i_{tar}$へ引き付ける力を表す係数である．
  また，$k_{rover} (1 < k_{rover})$は経由地点を追加した新しい経路の距離の，最短距離からの増加率である．
  これらの条件を満たすノード集合を$V^i_{sub}$とすると，経由するノード$v^i_{subgoal}$は以下の式で決められる．
  %
  \begin{equation}
    v^i_{subgoal} \gets \argmax_{v \in V^i_{sub}} EL^i_t(v)
  \end{equation}
  %

  \subsection{AMTDS with learning of dirt accumulation (AMTDS/LD)}

  \subsection{AMTDS with learning of event probabilities and enhancing divisional cooperation (AMTDS/EDC)}
  
  \subsection{AMTDS for energy saving and cleanliness (AMTDS/ESC)}
  
  \subsubsection{要求充足の判断}
 
  \subsubsection{自己重要度評価}
  
  \subsubsection{帰還動作 (Homing)}
  
  \subsubsection{待機動作 (Pausing)}
  
  \section{提案手法}
  
  \subsection{AMTDS for energy saving under the requirement (AMTDS/ER)}
  
  \subsubsection{HomingとPausingの組み合わせ}
  
  \subsubsection{補正係数$K$の導入}
  
  \subsubsection{未来のイベント発生量の予測}
  
  \subsection{AMTDS for energy saving under the requirement with learning of  event probabilities (AMTDS/ERL)}
  
  \subsubsection{イベント発生量の予測に使用するノードの範囲の変更}
  
  \subsubsection{補正係数$K^i$の更新方法の変更}
  
  \section{評価実験}

  \subsection{実験環境}
  
  \begin{table}
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{エージェントに関するパラメータ}
      \begin{tabular}{lcr} \\ \hline
        種類 & パラメーター & 値 \\ \hline
        エージェント数 &  |A| & 20 \\ \hline
        バッテリ & $B^i_{max}$ & 900 \\
                   & $B^i_{drain}$ & 1 \\
                   & $k^i_{charge}$ & 3 \\ \hline
        経路生成戦略 & $d_{myopia}$ & 10 \\
                     & $k_{att}$ & 1.0 \\
                     & $k_{rover}$ & 1.2 \\ \hline
      \end{tabular}
      \label{tb:1}
    \end{minipage}
    %
    \hfill
    %
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{目標決定戦略のパラメーター}
      \begin{tabular}{lcrr} \\ \hline
        目標決定戦略 & パラメーター & 値 \\ \hline
        PGS & $N_g$ & 5 \\ \hline
        PI & $N_i$ & 5 \\ \hline
        BNPS & $\alpha$ & 0.1 \\
             & $d_{rad}$ & 15 \\ \hline
        AMTDS & $\alpha$ & 0.1 \\
              & $\varepsilon$ & 0.05 \\ \hline
        AMTDS/LD & $\beta$ & 0.1 \\ \hline 
      \end{tabular}
      \label{tb:2}
    \end{minipage}
    %
    \vskip\baselineskip
    %
    \begin{minipage}[t]{.55\textwidth}
      \centering
      \caption{エネルギー節約行動に関するパラメーター}
      \begin{tabular}{lcrr} \\ \hline
        種類 & パラメーター & 値 \\ \hline
        自己重要度評価 & $T_s$ & 20 \\
                       & $T_l$ & 50 \\
                       & $T_f$ & 10 \\ \hline
        Homing & $T_{check}$ & 100 \\
               & $k_{homing}$ & $1/3$ \\ \hline
        Pausing & $T_{basic}$ & 100 \\ \hline  
        AMTDS/ERL & $T_{hp}$ & 500,000 \\ \hline       
      \end{tabular}
      \label{tb:3}
    \end{minipage}
  \end{table}

  \subsection{AMTDS/ERについての実験結果}
  \label{result_ER}
  
  \subsubsection{実験1: 性能評価}
  \label{ex:ER1}
  
  \subsubsection{実験2: 環境の変化による性能の違い}
  \label{ex:ER2}

  \subsubsection{実験3: エージェント数減少による性能の変化}

  \subsubsection{実験4: $K^i$の降順にエージェント数を減少させたときの性能の変化}

  \subsection{AMTDS/ERCについての実験結果}
  \label{result_ERC}
  
  \subsubsection{実験5: 性能評価}
  \label{ex:ERC1}
  
  \subsubsection{実験6: 環境の変化による性能の違い}
  \label{ex:ERC2}  
  
  \subsubsection{実験7: エージェント数減少による性能の変化}

  \subsubsection{実験8: $K^i$の降順にエージェント数を減少させたときの性能の変化}

  \section{結論}

  \clearpage
  \bibliographystyle{junsrt}
  \bibliography{ref}

\end{document}